{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN training\n",
    "This notebook demonstrates how to use classes from `opensoundscape.torch.models.cnn` and architectures created using `opensoundscape.torch.architectures.cnn_architectures` to\n",
    "\n",
    "- choose between single-target and multi-target model behavior\n",
    "\n",
    "- modify learning rates, learning rate decay schedule, and regularization\n",
    "\n",
    "- choose from various CNN architectures\n",
    "\n",
    "- train a multi-target model with a special loss function\n",
    "\n",
    "- use strategic sampling for imbalanced training data\n",
    "\n",
    "- customize preprocessing: train on spectrograms with a bandpassed frequency range\n",
    "\n",
    "Rather than demonstrating their effects on training (model training is slow!), most examples in this notebook either don't train the model or \"train\" it for 0 epochs for the purpose of demonstration.\n",
    "\n",
    "For introductory demos (basic training, prediction, saving/loading models), see the [\"Beginner-friendly training and prediction with CNNs\" tutorial](tutorials/cnn.html) (cnn.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess import preprocessors\n",
    "from opensoundscape.torch.models import cnn\n",
    "from opensoundscape.torch.architectures import cnn_architectures\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random \n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for big visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "### Download labeled audio files\n",
    "\n",
    "The Kitzes Lab has created a small labeled dataset of short clips of American Woodcock vocalizations. You have two options for obtaining the folder of data, called `woodcock_labeled_data`:\n",
    "\n",
    "1. Run the following cell to download this small dataset. These commands require you to have `tar` installed on your computer, as they will download and unzip a compressed file in `.tar.gz` format. \n",
    "\n",
    "2. Download a `.zip` version of the files by clicking [here](https://pitt.box.com/shared/static/m0cmzebkr5qc49q9egxnrwwp50wi8zu5.zip). You will have to unzip this folder and place the unzipped folder in the same folder that this notebook is in.\n",
    "\n",
    "If you already have these files, you can skip or comment out this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100     7    0     7    0     0      3      0 --:--:--  0:00:02 --:--:--     3\n",
      "100 9499k  100 9499k    0     0  2648k      0  0:00:03  0:00:03 --:--:-- 8659k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['rm', 'woodcock_labeled_data.tar.gz'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl','https://pitt.box.com/shared/static/79fi7d715dulcldsy6uogz02rsn5uesd.gz','-L', '-o','woodcock_labeled_data.tar.gz']) # Download the data\n",
    "subprocess.run([\"tar\",\"-xzf\", \"woodcock_labeled_data.tar.gz\"]) # Unzip the downloaded tar.gz file\n",
    "subprocess.run([\"rm\", \"woodcock_labeled_data.tar.gz\"]) # Remove the file after its contents are unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe of files and one-hot labels\n",
    "\n",
    "We need a dataframe with file paths in the index, so we manipulate the included one_hot_labels.csv slightly\n",
    "\n",
    "See the \"Basic training and prediction with CNNs\" tutorial for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>present</th>\n",
       "      <th>absent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/d4c40b6066b489518f8da83af1ee4984.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/79678c979ebb880d5ed6d56f26ba69ff.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/49890077267b569e142440fa39b3041c.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/0c453a87185d8c7ce05c5c5ac5d525dc.wav</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    present  absent\n",
       "file                                                               \n",
       "./woodcock_labeled_data/d4c40b6066b489518f8da83...        1       0\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e...        0       1\n",
       "./woodcock_labeled_data/79678c979ebb880d5ed6d56...        1       0\n",
       "./woodcock_labeled_data/49890077267b569e142440f...        1       0\n",
       "./woodcock_labeled_data/0c453a87185d8c7ce05c5c5...        1       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load one-hot labels dataframe\n",
    "labels = pd.read_csv('./woodcock_labeled_data/one_hot_labels.csv').set_index('file')\n",
    "# prepend the folder location to the file paths\n",
    "labels.index = pd.Series(labels.index).apply(lambda f: './woodcock_labeled_data/'+f)\n",
    "#create class list\n",
    "classes = labels.columns\n",
    "#inspect\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and validation sets\n",
    "\n",
    "Randomly split the data into training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created train_df (len 23) and valid_df (len 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(labels, test_size=0.2, random_state=0)\n",
    "print(f\"created train_df (len {len(train_df)}) and valid_df (len {len(valid_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model\n",
    "We initialize a model object by specifying the architecture, a list of classes, and the duration of individual samples in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/SML161/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "24.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch = cnn_architectures.resnet50(num_classes=len(classes))\n",
    "model = cnn.CNN(arch,classes,sample_duration=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can specify the name of an architecture as a string (see Cnn Architectures below for details or use `cnn_architectures.list_architectures()` for options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.CNN('resnet18',classes,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-target versus multi-target\n",
    "One important decision is whether your model is single-target (exactly one label per sample) or multi-target (any number of labels per sample, including 0). Single-target models have a softmax activation layer which forces the sum of all class scores to be 1.0. By default, models are created as multi-target, but you can set `single_target=True` either when creating the object or afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the model to be single_target\n",
    "model.single_target = True\n",
    "\n",
    "#or specify single_target when you create the object\n",
    "model = cnn.CNN(arch,classes,2.0,single_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training parameters\n",
    "We can modify various parameters about model training, including:\n",
    "\n",
    "* The learning rate\n",
    "* The learning rate schedule\n",
    "* Weight decay for regularization\n",
    "\n",
    "Let's take a peek at the current parameters, stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.optimizer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates\n",
    "The learning rate determines how much the model's weights change every time it calculates the loss function. \n",
    "\n",
    "Faster learning rates improve the speed of training and help the model leave local minima as it learns to classify, but if the learning rate is too fast, the model may not successfully fit the data or its fitting might be unstable.\n",
    "\n",
    "Often after training a model for a while at a relatively high learning rate (think 0.01), we might want to \"fine tune\" the model by training for a few epochs with a lower learning rate. Let's set a low learning rate for fine tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.optimizer_params['lr']=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate learning rates for feature and classifier blocks\n",
    "\n",
    "For ResNet architectures, we can modify the learning rates for the feature extration and classification blocks of the network separately. For example, we can specify a relatively fast learning rate for classifier and slower one for features, if we think the features from a pre-trained model are close to optimal but we have a different set of classes than the pre-trained model. We first use a helper function to separate the feature and classifier parameters, then specify parameters for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import separate_resnet_feat_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r18_model = cnn.CNN('resnet18',classes,2.0)\n",
    "print(r18_model.optimizer_params)\n",
    "\n",
    "separate_resnet_feat_clf(r18_model) #in place operation!\n",
    "\n",
    "#now we can specify separate parameters for the 'feature' and 'classifier' portions of the network\n",
    "r18_model.optimizer_params['feature']['lr'] = 0.001\n",
    "r18_model.optimizer_params['classifier']['lr'] = 0.01\n",
    "\n",
    "r18_model.optimizer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate schedule\n",
    "It's often helpful to decrease the learning rate over the course of training. By reducing the amount that the model's weights are updated as time goes on, this causes the learning to gradually switch from coarsely searching across possible weights to fine-tuning the weights.\n",
    "\n",
    "By default, the learning rates are multiplied by 0.7 (the learning rate \"cooling factor\") once every 10 epochs (the learning rate \"update interval\"). \n",
    "\n",
    "Let's modify that for a very fast training schedule, where we want to multiply the learning rates by 0.1 every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lr_cooling_factor = 0.1\n",
    "model.lr_update_interval = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization weight decay\n",
    "Pytorch optimizers perform [L2 regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization), giving the optimizer an incentive for the model to have small weights rather than large weights. The goal of this regularization is to reduce overfitting to the training data by reducing the complexity of the model. \n",
    "\n",
    "Depending on how much emphasis you want to place on the L2 regularization, you can change the weight decay parameter. By default, it is 0.0005. The higher the value for the \"weight decay\" parameter, the more the model training algorithm prioritizes smaller weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer_params['weight_decay']=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting CNN architectures\n",
    "\n",
    "The  [`opensoundscape.torch.architectures.cnn_architectures`](https://github.com/kitzeslab/opensoundscape/blob/master/opensoundscape/torch/architectures/cnn_architectures.py) module provides functions to create several common CNN architectures. These architectures are built in to pytorch, but the OpenSoundscape module helps us out by reshaping the final layer to match the number of classes we have. \n",
    "\n",
    "You could also create a custom architecture by subclassing an existing pytorch model or writing one from scratch (the minimum requirement is that it subclasses `torch.nn.Module` - it should at least have `.forward()` and `.backward()` methods.\n",
    "\n",
    "In general, we can create any pytorch model architecture and pass it to the `architecture` argument when creating a model in opensoundscape. We can choose whether to use pre-trained (ImageNet) weights or start from scratch (`use_pretrained=False` for random weights). For instance, lets create an alexnet architecture with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arch = cnn_architectures.alexnet(num_classes=len(classes),use_pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can also initialize a model object by providing the name of an architecture as a string, rather than the architecture object. For a list of valid architecture names, use `cnn_architectures.list_architectures()`. Note that these will use default architecture parameters, including using pre-trained ImageNet weights. If you don't want to use pre-trained weights, follow the method above of creating the architecture and passing it to the initialization of CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_architectures.list_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.CNN(architecture='resnet18',classes=classes, sample_duration=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained weights\n",
    "\n",
    "In OpenSoundscape, by default, model architectures are initialized with weights pretrained on the [ImageNet](https://www.image-net.org/) image database. It takes some time for pytorch to download these weights from an online repository the first time an instance of a particular architecture is created with pretrained weights - pytorch will do this automatically and only once.\n",
    "\n",
    "Using pretrained weights often speeds up training significantly, as the representation learned from ImageNet is a good start at beginning to interpret spectrograms, even though they are not true \"pictures.\"\n",
    "\n",
    "If you prefer not to use pre-trained weights, or if you don't have an internet connection, you can specify `use_pretrained` argument to `False`, when creating an architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arch = cnn_architectures.alexnet(num_classes=10,use_pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the feature extractor\n",
    "\n",
    "Convolutional Neural Networks can be thought of as having two parts: a **feature extractor** which learns how to represent/\"see\" the input data, and a **classifier** which takes those representations and transforms them into predictions about the class identity of each sample.\n",
    "\n",
    "You can freeze the feature extractor if you only want to train the final classification layer of the network but not modify any other weights. This could be useful for applying pre-trained classifiers to new data, i.e. \"transfer learning\". To do so, set the `freeze_feature_extractor` argument to `True` when you create an architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See \"InceptionV3 architecture\" section below for more information\n",
    "arch = cnn_architectures.resnet50(num_classes=10, freeze_feature_extractor=True, use_pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3 class\n",
    "The Inception architecture requires slightly different training and preprocessing from the ResNet architectures and the other architectures implemented in OpenSoundscape (see below), because:\n",
    "\n",
    "1) the input image shape must be 299x299, and\n",
    "\n",
    "2) Inception's forward pass gives output + auxiliary output instead of a single output\n",
    "\n",
    "The InceptionV3 class in `cnn` handles the necessary modifications in training and prediction for you, so use that instead of CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import InceptionV3\n",
    "\n",
    "#generate an Inception model\n",
    "model = InceptionV3(classes=classes,use_pretrained=False,sample_duration=2)\n",
    "\n",
    "#train and validate for 1 epoch\n",
    "#note that Inception will complain if batch_size=1\n",
    "model.train(train_df,valid_df,epochs=1,batch_size=4)\n",
    "\n",
    "#predict\n",
    "preds, _, _ = model.predict(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Changing the architecture of an existing model (not recommended)\n",
    "The architecture is stored in the model object's `.newtork` attribute. We can access parameters of the network or even replace it entirely. \n",
    "\n",
    "Note that replacing the architecture will completely remove anything the model has \"learned\" since the learned weights are a part of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the AlexNet architecture\n",
    "new_arch = cnn_architectures.densenet121(num_classes=2, use_pretrained=False)\n",
    "\n",
    "# replace the alexnet architecture with the densenet architecture\n",
    "model.network = new_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-target training with ResampleLoss\n",
    "\n",
    "Training multi-target models (a.k.a. multi-label: there can be any number of positive labels on each sample) is challenging and can benefit from using a modified loss function. OpenSoundscape provides a loss function designed for training multi-target models. We recommend using this loss function when training multi-target models. You can add it to a class with an in-place helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import use_resample_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.CNN('resnet18',classes,2.0)\n",
    "use_resample_loss(model)\n",
    "print(model.loss_cls)\n",
    "\n",
    "#use as normal...\n",
    "#model.train(...)\n",
    "#model.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicting with custom preprocessors\n",
    "The preprocessing tutorial gives in-depth descriptions of how to customize your preprocessing pipeline.\n",
    "\n",
    "Here, we'll just give a quick example of tweaking the preprocessing pipeline: providing the CNN with a bandpassed spectrogram object instead of the full frequency range. \n",
    "\n",
    "It's good practice to create the validation from the training dataset (after any modifications are made), so that they perform the same preprocessing. You may or may not want to use augmentation on the validation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training on bandpassed spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.CNN('resnet18', classes, 2.0)\n",
    "\n",
    "# change the min and max frequencies for the spectrogram bandpass action\n",
    "model.preprocessor.pipeline.bandpass.set(min_f=3000, max_f=5000)\n",
    "\n",
    "# inspect a few preprocessed samples (see basic CNN training and prediction tutorial for details)\n",
    "from opensoundscape.preprocess.utils import show_tensor_grid\n",
    "from opensoundscape.torch.datasets import AudioFileDataset\n",
    "sample_of_4 = train_df.sample(n=4)\n",
    "inspection_dataset = AudioFileDataset(sample_of_4, model.preprocessor) \n",
    "samples = [sample['X'] for sample in inspection_dataset]\n",
    "labels = [sample['y'] for sample in inspection_dataset]\n",
    "_ = show_tensor_grid(samples,4,labels=labels)\n",
    "\n",
    "# now we can train and validate on the bandpassed spectrograms\n",
    "model.train(train_df, valid_df, epochs=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we predict using this model for prediction, it will use the same preprocessor settings, bandpassing the prediction samples in the same way as the training samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean up\n",
    "remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./woodcock_labeled_data')\n",
    "\n",
    "for p in Path('.').glob('*.model'):\n",
    "    p.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_dev",
   "language": "python",
   "name": "opso_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
